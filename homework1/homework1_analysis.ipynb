{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 1 — Finding Textually Similar Documents\n",
    "\n",
    "### Objective\n",
    "\n",
    "In this assignment, we implement a system to **find textually similar documents** using:\n",
    "- **Shingling** — converting text into sets of substrings,\n",
    "- **MinHashing** — efficiently estimating Jaccard similarity,\n",
    "- **Locality-Sensitive Hashing (LSH)** — identifying potentially similar document pairs.\n",
    "\n",
    "We will test the system on a **small corpus of English recipe texts**,  \n",
    "each describing a different dish (e.g., pasta, risotto, paella, soup, etc.).  \n",
    "By comparing them, we can observe how the algorithms detect similarity\n",
    "between recipes that share ingredients, structure, or vocabulary.\n",
    "\n",
    "---\n",
    "### Pipeline Summary\n",
    "\n",
    "1. **Load and preprocess** text data from files in the `data/` directory.\n",
    "2. **Create k-shingles** for each document.\n",
    "3. **Compute exact Jaccard similarities** between all pairs of documents.\n",
    "4. **Generate MinHash signatures** and compare them to estimate similarity.\n",
    "5. **Optionally apply LSH** to find similar documents more efficiently.\n",
    "6. **Discuss results** and interpret which recipes are most similar.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# --- Setup and imports ---\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Add src/ folder to path for module imports\n",
    "sys.path.append(\"../src\")\n",
    "\n",
    "# Import our custom functions\n",
    "from shingling import create_shingles, hash_shingles\n",
    "from compare_sets import jaccard_similarity\n",
    "from minhashing import generate_hash_functions, compute_minhash_signature\n",
    "from compare_signatures import signature_similarity\n",
    "from lsh import lsh_candidate_pairs\n",
    "\n",
    "# Optional: for visualizations later\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"Modules successfully imported.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading and Inspecting the Dataset\n",
    "\n",
    "We first load our **corpus of recipes** from the `data/` folder.  \n",
    "Each file contains one recipe written in English — for example:\n",
    "- *Pasta Carbonara*\n",
    "- *Risotto alla Milanese*\n",
    "- *Chicken Soup*\n",
    "- *Spanish Paella*\n",
    "- *Beef Stew*\n",
    "- *Vegetable Curry*\n",
    "\n",
    "By using texts from **different cuisines and types of dishes**,  \n",
    "we can later verify whether the similarity detection methods\n",
    "capture linguistic and semantic overlap (e.g., recipes for pasta may\n",
    "be more similar to each other than to soups or desserts).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# --- Load all text files from the data directory ---\n",
    "\n",
    "data_path = \"../data/\"\n",
    "documents = []\n",
    "filenames = []\n",
    "\n",
    "for fname in sorted(os.listdir(data_path)):\n",
    "    if fname.endswith(\".txt\"):\n",
    "        with open(os.path.join(data_path, fname), \"r\", encoding=\"utf-8\") as f:\n",
    "            documents.append(f.read())\n",
    "            filenames.append(fname)\n",
    "\n",
    "print(f\"{len(documents)} documents loaded.\\n\")\n",
    "print(\"Document names:\", filenames)\n",
    "print(\"\\nSample text from the first recipe:\\n\")\n",
    "print(documents[0][:400])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Shingling: Representing Documents as Sets\n",
    "\n",
    "**Shingling** is the process of breaking each document into overlapping  \n",
    "substrings (called *k-shingles*) of a fixed length `k`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# --- Create and hash k-shingles ---\n",
    "\n",
    "k = 10  # shingle length (characters)\n",
    "shingle_sets = []\n",
    "\n",
    "for doc in documents:\n",
    "    shingles = create_shingles(doc, k)\n",
    "    hashed = hash_shingles(shingles)\n",
    "    shingle_sets.append(hashed)\n",
    "\n",
    "print(f\"Created {len(shingle_sets)} shingle sets (one per recipe).\")\n",
    "print(f\"Example shingles from first document: {list(create_shingles(documents[0], 10))[:10]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "n = len(shingle_sets)\n",
    "jaccard_matrix = [[0]*n for _ in range(n)]\n",
    "\n",
    "for i in range(n):\n",
    "    for j in range(n):\n",
    "        if i < j:\n",
    "            sim = jaccard_similarity(shingle_sets[i], shingle_sets[j])\n",
    "            jaccard_matrix[i][j] = jaccard_matrix[j][i] = sim\n",
    "\n",
    "# Convert to DataFrame for readability\n",
    "df_jaccard = pd.DataFrame(jaccard_matrix, index=filenames, columns=filenames)\n",
    "\n",
    "print(\"Exact Jaccard similarity matrix:\")\n",
    "df_jaccard.round(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# --- Visualize Jaccard similarities ---\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.heatmap(df_jaccard, annot=True, fmt=\".2f\", cmap=\"Blues\")\n",
    "plt.title(\"Exact Jaccard Similarity Between Recipes\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. MinHashing: Efficient Similarity Estimation\n",
    "\n",
    "Computing Jaccard similarity directly can be slow for large datasets.  \n",
    "**MinHashing** provides a way to approximate it efficiently.\n",
    "\n",
    "The key idea:\n",
    "- We define several random hash functions.\n",
    "- Each hash function maps the shingles of a document to integers.\n",
    "- The **minimum hash value** per function is recorded.\n",
    "- The resulting vector of minimum values is the document’s **MinHash signature**.\n",
    "\n",
    "The similarity between two signatures (fraction of equal positions)\n",
    "approximates their Jaccard similarity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "num_hashes = 100\n",
    "max_shingle_id = 2**32 - 1  # modulus for hash functions\n",
    "\n",
    "# Generate random hash functions\n",
    "hash_functions = generate_hash_functions(num_hashes, max_shingle_id)\n",
    "\n",
    "# Compute signatures for all documents\n",
    "signatures = []\n",
    "for s in shingle_sets:\n",
    "    sig = compute_minhash_signature(s, hash_functions, max_shingle_id)\n",
    "    signatures.append(sig)\n",
    "\n",
    "print(f\"Computed {len(signatures)} MinHash signatures (length {num_hashes} each).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# --- Estimate similarity using MinHash signatures ---\n",
    "\n",
    "minhash_matrix = [[0]*n for _ in range(n)]\n",
    "\n",
    "for i in range(n):\n",
    "    for j in range(n):\n",
    "        if i < j:\n",
    "            sim = signature_similarity(signatures[i], signatures[j])\n",
    "            minhash_matrix[i][j] = minhash_matrix[j][i] = sim\n",
    "\n",
    "df_minhash = pd.DataFrame(minhash_matrix, index=filenames, columns=filenames)\n",
    "\n",
    "print(\"Estimated similarities using MinHash:\")\n",
    "df_minhash.round(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# --- Visualize MinHash-based similarity ---\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.heatmap(df_minhash, annot=True, fmt=\".2f\", cmap=\"Greens\")\n",
    "plt.title(\"Estimated Similarity Using MinHash\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Locality-Sensitive Hashing (LSH)\n",
    "\n",
    "To further reduce comparisons between documents,  \n",
    "**Locality-Sensitive Hashing (LSH)** groups signatures into bands of rows.\n",
    "\n",
    "Each band is hashed into a *bucket* — documents landing in the same bucket\n",
    "are considered *candidate pairs*, likely to be similar.\n",
    "\n",
    "This technique is widely used for large-scale similarity search\n",
    "(e.g., near-duplicate detection on the web).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "bands = 20\n",
    "rows = 5  # 20 * 5 = 100 (same as number of hash functions)\n",
    "\n",
    "candidates = lsh_candidate_pairs(signatures, bands, rows)\n",
    "\n",
    "print(\"Candidate pairs identified using LSH:\")\n",
    "for (i, j) in candidates:\n",
    "    print(f\"{filenames[i]}  <-->  {filenames[j]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Discussion and Interpretation\n",
    "\n",
    "The results highlight how documents with overlapping wording or similar\n",
    "content structure tend to show higher similarity values.\n",
    "\n",
    "Once the recipe texts are finalized, this section should describe which\n",
    "types of recipes appear most similar. For example:\n",
    "- `[Recipe A]` and `[Recipe B]` might show high similarity due to shared\n",
    "  ingredients or preparation steps.\n",
    "- `[Recipe C]` might differ significantly because it uses a different cooking\n",
    "  style or vocabulary.\n",
    "\n",
    "When the actual dataset is available, you can replace these placeholders\n",
    "with concrete observations based on the similarity matrices or LSH results.\n",
    "\n",
    "---\n",
    "\n",
    "### Insights to Include Later\n",
    "\n",
    "- Which documents form small *clusters* of similar recipes?\n",
    "- Are there recipes that are unexpectedly similar or dissimilar?\n",
    "- Does the similarity correspond to the *type of dish*, *ingredients*, or *cooking method*?\n",
    "\n",
    "---\n",
    "\n",
    "### Scalability and Method Comparison\n",
    "\n",
    "| Method | Description | Pros | Cons |\n",
    "|:-------|:-------------|:------|:------|\n",
    "| **Jaccard Similarity** | Exact overlap between sets | Intuitive and precise | Slow for large datasets |\n",
    "| **MinHashing** | Approximation of Jaccard using random hashes | Fast, scalable | Slight approximation error |\n",
    "| **LSH** | Groups similar signatures into candidate buckets | Very efficient for big data | May produce false positives |\n",
    "\n",
    "---\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "- **Shingling** transforms documents into comparable feature sets.  \n",
    "- **Jaccard similarity** provides the ground truth of overlap.  \n",
    "- **MinHashing** offers a scalable approximation with similar results.  \n",
    "- **LSH** helps to quickly identify potential matches without\n",
    "  exhaustively comparing every pair.\n",
    "\n",
    "When the dataset is complete, these methods will reveal\n",
    "how textual similarity reflects common patterns in the recipe corpus."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
